{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "N_9CaGLjs_Sb",
        "Qm1Gp2-LrEym"
      ],
      "authorship_tag": "ABX9TyO2792A5pYP4BYKaKyj0a+k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grosetta/rdkit_study/blob/main/train_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewf5K0tnqHT_",
        "outputId": "d7784a9c-5b15-46f7-9601-a82035764a60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#文件头代码\n",
        "import sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "sys.path.insert(0,\"/content/gdrive/My Drive/Colab Notebooks/python_package/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 模块导入"
      ],
      "metadata": {
        "id": "N_9CaGLjs_Sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import itertools\n",
        "import operator\n",
        "import numpy as np\n",
        "import nltk\n",
        "import os\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "from IPython import display\n",
        "from rdkit.Chem import Descriptors\n",
        "import sys\n",
        "from datetime import datetime\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation,TimeDistributed\n",
        "from keras.layers import LSTM,GRU\n",
        "from keras.layers import Embedding\n",
        "from keras.optimizers import RMSprop, Adam\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.layers import Dropout\n",
        "import random\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import model_from_json\n",
        "from keras.layers import Conv1D, MaxPooling1D"
      ],
      "metadata": {
        "id": "TVzS2kFWqXUW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 函数定义"
      ],
      "metadata": {
        "id": "ouINmz0DrCVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def zinc_data_with_bracket_original():\n",
        "#将文件的所有smile转存为一个列表，列表中每一个元素是一个smile\n",
        "    sen_space=[]\n",
        "    f = open('/content/gdrive/My Drive/data/250k_rndm_zinc_drugs_clean.smi', 'r')\n",
        "\n",
        "    reader = csv.reader(f)\n",
        "    for row in reader:\n",
        "        sen_space.append(row) #此时的sen_space是列表里套列表的形式\n",
        "    f.close()\n",
        "\n",
        "    zinc_processed=[]\n",
        "    for i in range(len(sen_space)):\n",
        "        word1=sen_space[i]\n",
        "        zinc_processed.append(word1[0])\n",
        "    return zinc_processed"
      ],
      "metadata": {
        "id": "Z4XpoOsYrEgV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zinc_processed_with_bracket(sen_space):\n",
        "    all_smile=[]\n",
        "    length=[]\n",
        "    end=\"\\n\"\n",
        "    element_table=[\"C\",\"N\",\"B\",\"O\",\"P\",\"S\",\"F\",\"Cl\",\"Br\",\"I\",\"(\",\")\",\"=\",\"#\"]\n",
        "    ring=[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"]\n",
        "\n",
        "    for i in range(len(sen_space)):\n",
        "        word_space=sen_space[i]\n",
        "        word=[]\n",
        "        j=0\n",
        "        while j<len(word_space):\n",
        "            word_space1=[]\n",
        "            if word_space[j]==\"[\":\n",
        "                word_space1.append(word_space[j])\n",
        "                j=j+1\n",
        "                while word_space[j]!=\"]\":\n",
        "                    word_space1.append(word_space[j])\n",
        "                    j=j+1\n",
        "                word_space1.append(word_space[j])\n",
        "                word_space2=''.join(word_space1)\n",
        "                word.append(word_space2)    #将smile的中括号即其内部的字母作为整体添加到word列表中\n",
        "                j=j+1\n",
        "            else:\n",
        "                word_space1.append(word_space[j])\n",
        "\n",
        "                if j+1<len(word_space):\n",
        "                    word_space1.append(word_space[j+1])  #如果第j个字符不是最后一个，那就继续遍历\n",
        "                    word_space2=''.join(word_space1)\n",
        "                else:\n",
        "                    word_space1.insert(0,word_space[j-1]) #如果第j个字符是最后一个\n",
        "                    word_space2=''.join(word_space1)\n",
        "\n",
        "                if word_space2 not in element_table:   #如果第j个字符不在预定的元素集合里，就照搬到word列表中 \n",
        "                    word.append(word_space[j])\n",
        "                    j=j+1\n",
        "                else:\n",
        "                    word.append(word_space2)\n",
        "                    j=j+2\n",
        "\n",
        "\n",
        "        word.append(end)  #给每个列表元素最后加终止符\n",
        "        word.insert(0,\"&\") #给每个列表第一个元素前加&\n",
        "        len1=len(word)\n",
        "        length.append(len1) #每个元素（smile各字符隔开）长度添加到一个列表中，和all_smile中一一对应。\n",
        "        all_smile.append(list(word))\n",
        "    val=[\"\\n\"]\n",
        "    for i in range(len(all_smile)):\n",
        "        for j in range(len(all_smile[i])):   #j的取值范围各不相同，受制于各smile长度\n",
        "            if all_smile[i][j] not in val:\n",
        "                val.append(all_smile[i][j])\n",
        "\n",
        "    return val, all_smile"
      ],
      "metadata": {
        "id": "qCjyOrjMsDF9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(smiles,all_smile):\n",
        "    all_smile_index=[]\n",
        "    for i in range(len(all_smile)):\n",
        "        smile_index=[]\n",
        "        for j in range(len(all_smile[i])):\n",
        "            smile_index.append(smiles.index(all_smile[i][j]))  #index() 函数用于从列表中找出某个值第一个匹配项的索引位置\n",
        "        all_smile_index.append(smile_index)\n",
        "    X_train=all_smile_index         #X_train表示smile中符号在valcabulary中对应的序号\n",
        "    y_train=[]\n",
        "    for i in range(len(X_train)):\n",
        "\n",
        "        x1=X_train[i]    #smile转为由valcabulary中对应序号组成的列表\n",
        "        x2=x1[1:len(x1)]    \n",
        "        x2.append(0)\n",
        "        y_train.append(x2)\n",
        "\n",
        "    return X_train,y_train #y_train和X_train错开一位"
      ],
      "metadata": {
        "id": "0zc5Pr-7ELWx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#定义训练模型的保存函数\n",
        "def save_model(model):\n",
        "    # serialize model to JSON\n",
        "    model_json = model.to_json()\n",
        "    with open(\"model.json\", \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "    # serialize weights to HDF5\n",
        "    model.save_weights(\"model.h5\")\n",
        "    print(\"Saved model to disk\")"
      ],
      "metadata": {
        "id": "HVrTdAYFtQlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 代码主体"
      ],
      "metadata": {
        "id": "Qm1Gp2-LrEym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smile=zinc_data_with_bracket_original()\n",
        "valcabulary,all_smile=zinc_processed_with_bracket(smile)\n",
        "print(valcabulary)\n",
        "print(len(all_smile))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4FNd04prG11",
        "outputId": "84c9abf0-2ca7-4bba-c5e4-fc382c7406ca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', '&', 'C', '(', ')', 'c', '1', '2', 'o', '=', 'O', 'N', '3', 'F', '[C@@H]', 'n', '-', '#', 'S', 'Cl', '[O-]', '[C@H]', '[NH+]', '[C@]', 's', 'Br', '/', '[nH]', '[NH3+]', '4', '[NH2+]', '[C@@]', '[N+]', '[nH+]', '\\\\', '[S@]', '5', '[N-]', '[n+]', '[S@@]', '[S-]', '6', '7', 'I', '[n-]', 'P', '[OH+]', '[NH-]', '[P@@H]', '[P@@]', '[PH2]', '[P@]', '[P+]', '[S+]', '[o+]', '[CH2-]', '[CH-]', '[SH+]', '[O+]', '[s+]', '[PH+]', '[PH]', '8', '[S@@+]']\n",
            "249456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " X_train,y_train=prepare_data(valcabulary,all_smile)"
      ],
      "metadata": {
        "id": "iX48CF76vM6q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#maxlen：None或整数，为序列的最大长度。大于此长度的将被截断，小于此长度的序列将在后面填0.\n",
        "#padding：pre或post，确定当需要补0时，在序列的起始还是结尾补。\n",
        "#truncating：pre或post，确定需要截断序列时，从起始还是结尾截断。\n",
        "#value：浮点数，用于填充序列\n",
        "X= pad_sequences(X_train, maxlen=81, dtype='int32', padding='post', truncating='pre', value=0.)\n",
        "y = pad_sequences(y_train, maxlen=81, dtype='int32', padding='post', truncating='pre', value=0.)"
      ],
      "metadata": {
        "id": "UC7oDbVUcS7m"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_one_hot = np.array([to_categorical(sent_label, num_classes=len(valcabulary)) for sent_label in y])\n",
        "print (y_train_one_hot.shape)\n",
        "\n",
        "vocab_size=len(valcabulary)\n",
        "embed_size=len(valcabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6Aic8_vpaO0",
        "outputId": "d482f361-7648-487f-9f54-5b8d74081496"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(249456, 81, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "N=X.shape[1]\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=len(valcabulary), input_length=N,mask_zero=False))\n",
        "model.add(GRU(256, input_shape=(81,64),activation='tanh',return_sequences=True))\n",
        "#model.add(LSTM(output_dim=256, input_shape=(81,64),activation='tanh',return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(GRU(256,activation='tanh',return_sequences=True))\n",
        "#model.add(LSTM(output_dim=1000, activation='sigmoid',return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(TimeDistributed(Dense(embed_size, activation='softmax')))\n",
        "optimizer=Adam(lr=0.01)\n",
        "print(model.summary())\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "ul6ZDdQ5jOcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X,y_train_one_hot,epochs=100, batch_size=512,validation_split=0.1)"
      ],
      "metadata": {
        "id": "aoNzU4Puqsjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_model(model)"
      ],
      "metadata": {
        "id": "rsB2uFS2qw1L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}